{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient in PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNR+aUXIBt3t+HzXk+BjYTF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M0315G/PyTorch-Basics/blob/main/Section1%20-%20Getting%20Started%20with%20PyTorch/Gradient_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnbJXW4-ekc5"
      },
      "source": [
        "### **Gradients in PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STngjTHD9eqW"
      },
      "source": [
        "Basically, a numerical gradient tells you in which direction we need to move when you are machine learning i.e. positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t2gpXqZ9etR"
      },
      "source": [
        "While the actual numerical value of the Gradient will tell you **how much** you should move. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac-ZFDVK95LD"
      },
      "source": [
        "In the context of **Machine Learning**, what we normally do is take a loop over a group of tensors and compute gradients for them and then update the values of those tensors that are being 'learned' using the **product of gradients** and the **learning rate.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDPze5GW-XEg"
      },
      "source": [
        "Mathematically speaking, a gradient is a numerical representation of a **derivative** -- which means we have to ask the question - **\"A gradient with respect to what?\"**\n",
        "\n",
        "\n",
        "And the answer is a **loss function**, in Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzYbitEn-8v4"
      },
      "source": [
        "Lets try it out with a simple algebra example\n",
        "\n",
        "``` X + 1 = 3 ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JXJ4Bkx7bnWH",
        "outputId": "58f7c847-0afd-4fde-c86e-cbc2a4f1cc97"
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.8.1+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzGsYyS8bsn4"
      },
      "source": [
        "We'll initialize our tensor with a random initial guess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65C-Lu5ObnY1"
      },
      "source": [
        "# Creating an Random Tensor\n",
        "\n",
        "X = torch.rand(1, requires_grad=True)  # Argument requires_grad is set to true so that we can update it based on the gradient."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2AxU8JfcNjX"
      },
      "source": [
        "And our formula is "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bgG9YNtcpE-",
        "outputId": "1fd08e9d-550a-4f2b-c66b-0333407a2c8c"
      },
      "source": [
        "Y = X + 1.0\n",
        "Y"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.9536], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tH66YI2cx3G"
      },
      "source": [
        "# Loss Function\n",
        "\n",
        "def mse(Y):\n",
        "  diff = 3.0 - Y\n",
        "  return (diff * diff).sum()/2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69xPqO0C_wuD"
      },
      "source": [
        "The gradient on our X tells us which direction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35VrQ02-cx6m",
        "outputId": "dbcf7f55-6ca3-4a64-f7fc-18dc6ba65944"
      },
      "source": [
        "loss =  mse(Y)\n",
        "loss.backward()\n",
        "X.grad"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.0464])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmi0_v-qdObO"
      },
      "source": [
        "Now, let's use the gradien to solve some algebra with simple Machine learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUR91oYJdGuP",
        "outputId": "8fd5eec3-8440-42c0-dad7-2c6b804360ef"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "# Training loop\n",
        "for i in range(0, 10000):\n",
        "  Y = X + 1.0\n",
        "\n",
        "  loss = mse(Y)\n",
        "  # Backpropogation of the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # here is the 'learning', so we turn off the gradients from being updated temporatily\n",
        "  with torch.no_grad():\n",
        "    # Gradient will tell you which direction you're off\n",
        "    # so you'll go in the opposite direction to correct the weights\n",
        "    X -= learning_rate * X.grad\n",
        "    # and we zero out the gradients to get fresh values\n",
        "    # on each learining loop iteration\n",
        "    X.grad.zero_()\n",
        "\n",
        "\n",
        "\n",
        "# Here is our answer\n",
        "X"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.9999], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}